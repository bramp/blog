<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Webcache on bramp.net</title>
    <link>https://blog.bramp.net</link>
    <description>Recent content in Webcache on bramp.net</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en_GB</language>
    <lastBuildDate>Fri, 23 Jul 2010 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://blog.bramp.net/tags/webcache/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>My rant about the university webcache</title>
      <link>https://blog.bramp.net/post/2010/07/23/my-rant-about-the-university-webcache/</link>
      <pubDate>Fri, 23 Jul 2010 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bramp.net/post/2010/07/23/my-rant-about-the-university-webcache/</guid>
      <description>

&lt;p&gt;I wrote this a while ago and posted it on the &lt;a href=&#34;http://www.facebook.com/group.php?gid=253120796980&#34;&gt;Get rid of the university webcache!&lt;/a&gt; Facebook group. I&amp;#8217;ve just been reminded of it and thought I&amp;#8217;ll post it here:&lt;/p&gt;

&lt;h4 id=&#34;get-rid-of-the-university-webcache&#34;&gt;Get rid of the university webcache!&lt;/h4&gt;

&lt;p&gt;As many of you know, to access any website on the Internet from campus you must configure your browser to use the webcache. This is both inconvenient and frustrating. I am a researcher in the computer science department and I have had enough of this annoyance. It regularly gets in the way of my work and I spend countless hours trying to work around it.&lt;/p&gt;

&lt;p&gt;Over the years I have asked members of ISS why they still have a webcache when many other University do not, and especially most computing departments do not. I have compiled a list of the common reasons, and what I think of them.&lt;/p&gt;

&lt;h4 id=&#34;reasons-for-the-webcache&#34;&gt;Reasons for the webcache&lt;/h4&gt;

&lt;h4 id=&#34;1-the-webcache-reduces-bandwidth-usage-by-caching-popular-items&#34;&gt;1. The webcache reduces bandwidth usage by caching popular items&lt;/h4&gt;

&lt;p&gt;Typically webcaches were deployed to save money by reducing the number of files fetched from the Internet. Each file a user views would be stored in the cache. If another user came along to view the same file the cache would serve the file from its local file system instead of going onto the Internet. This would reduce the bandwidth usage, as well as make the user&amp;rsquo;s web browsing experience faster.&lt;/p&gt;

&lt;p&gt;The problem with this however, is that in the last 5 years the Internet has changed drastically. Most websites can no longer be cached. For example, pages on Facebook (the #2 most popular site according to Alexa.com), can not be cached. Each user see pages customised for them depending on which friends they have. Additionally, the pages on Facebook never stay the same for long which is terrible for caching. The third most popular website, YouTube (again according to Alexa.com), is not cached by our webcache either. YouTube serves large videos which the webcache will not understand how to cache.&lt;/p&gt;

&lt;p&gt;Sure, there is still content which is cacheable. For example, static images on websites such as logos will not change offer and therefore be cached. However having a look at this URL: &lt;a href=&#34;http://wwwcache.lancs.ac.uk/cachestatus&#34;&gt;http://wwwcache.lancs.ac.uk/cachestatus&lt;/a&gt; shows how effective the caches actually are. There is a metric called Hit Rate, which is the percentage of requests (and bandwidth) that have been served locally instead of going to the Internet. Higher numbers are better, but looking at the hit rate now, it shows an average of 15% requests and 3% bandwidth. Overall 3% of bandwidth doesn&amp;rsquo;t seem very useful. In the past when the webcache was first deployed it was easy to get &amp;gt;60%.&lt;/p&gt;

&lt;h4 id=&#34;2-the-cache-allows-iss-to-monitor-what-people-are-doing-as-well-as-block-websites&#34;&gt;2. The cache allows ISS to monitor what people are doing, as well as block websites&lt;/h4&gt;

&lt;p&gt;The webcache is most likely configured to log every site people visit, allowing ISS to look though your history. I&amp;rsquo;m sure this is very useful for dealing with the many copyright notices the university receives after student&amp;rsquo;s download something inappropriate. In the past ISS have also used the webcache to blocked sites, for example, the infamous LUSerNet site. The only argument I can give to these benefits is that there are other ways to achieving logging and blocking which are just as simple, and I would be surprised if ISS weren&amp;rsquo;t using some of these techniques already.&lt;/p&gt;

&lt;h4 id=&#34;3-the-webcache-keeps-us-safe-from-viruses&#34;&gt;3. The webcache keeps us safe from viruses.&lt;/h4&gt;

&lt;p&gt;I was told that every file downloaded will be scanned for viruses, this attempts to keep your computer clean. I&amp;rsquo;m unsure if the webcache is actually able to scan for viruses that quickly, but if it is I&amp;rsquo;m unsure of its usefulness. Most desktop PCs (if not all) have some kind of virus scanner installed. Additionally the big viruses that have hit the news in recent years, such as Slammer, or Conficker would not have been stopped by the webcache as they do not spread over HTTP.&lt;/p&gt;

&lt;p&gt;As well as the advantages of the webcache there are many reasons against having webcaches.&lt;/p&gt;

&lt;h4 id=&#34;reasons-against-the-webcache&#34;&gt;Reasons against the webcache&lt;/h4&gt;

&lt;h4 id=&#34;1-reduced-redundancy&#34;&gt;1. Reduced redundancy&lt;/h4&gt;

&lt;p&gt;From what I understand the university has multiple Internet connections, and the network is set up in such a way that if all the cables running between the two ends of campus were cut, both halves would still have Internet connection. This sounds like a great thing in ensuring the network is always working. However, I&amp;rsquo;ve been told that the webcaches are in a single central location, completely negating the benefits of the network&amp;rsquo;s resilience. Because, if the link to the single location is cut the whole campus loses web access.&lt;/p&gt;

&lt;h4 id=&#34;2-inconvenient-to-configure-software&#34;&gt;2. Inconvenient to configure software&lt;/h4&gt;

&lt;p&gt;This is what annoys me the most, is that I have to ensure all the software I use can use a web proxy, and that all of it is configured to do so. As more and more applications start to use the Internet, it is not just web browsers that need to be configured. Things like, instant messengers, online music applications, software updaters, and many more all need the Internet. Every few months I have to spend my time hacking a solution to make it work behind the webcache.&lt;br /&gt;
There is also the added inconvenience that if you take a Laptop between home and university, you have to flip the cache settings on and off. This has caused many headaches for people that have forgotten to do so.&lt;/p&gt;

&lt;h4 id=&#34;alternatives&#34;&gt;Alternatives&lt;/h4&gt;

&lt;p&gt;Now most Universities have scrapped their webcaches because they just weren&amp;rsquo;t useful anymore. However, if Lancaster wishes to keep theirs they could implement it in a different way. A transparent proxy is one such solution which would greatly improve the user&amp;rsquo;s experience. Basically every time you try and visit a website the transparent proxy handles your request without the user even knowing. This means you don&amp;rsquo;t have to change any settings in your web browser! Transparent proxies have been used by large ISPs to handle 1000s of clients, so I don&amp;rsquo;t see why ISS couldn&amp;#8217;t do this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Redirect local traffic to a web cache with iptables</title>
      <link>https://blog.bramp.net/post/2010/01/26/redirect-local-traffic-to-a-web-cache-with-iptables/</link>
      <pubDate>Tue, 26 Jan 2010 00:00:00 +0000</pubDate>
      
      <guid>https://blog.bramp.net/post/2010/01/26/redirect-local-traffic-to-a-web-cache-with-iptables/</guid>
      <description>&lt;p&gt;Very occasionally I come across a Linux application that does not respect the http_proxy variable. This stops the application from working while I&amp;#8217;m at university as they forbid traffic on port 80 unless it goes via their webcache. So today I came up with a hack of a solution that involves iptables:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;c&#34;&gt;# IP address and port number of the webcache&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;WEBCACHE&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;194.80.32.10:8080

&lt;span class=&#34;c&#34;&gt;# Flush any previous rules&lt;/span&gt;
iptables -t nat --flush

&lt;span class=&#34;c&#34;&gt;# Delete and recreate the chain&lt;/span&gt;
iptables -t nat -X HTTPFORCE
iptables -t nat -N HTTPFORCE

&lt;span class=&#34;c&#34;&gt;# Don&amp;#39;t touch local traffic (localhost and internal network)&lt;/span&gt;
iptables -t nat -A HTTPFORCE -o lo -j RETURN
iptables -t nat -A HTTPFORCE --dst 127.0.0.1/8 -j RETURN
iptables -t nat -A HTTPFORCE --dst 10.0.0.0/8 -j RETURN
&lt;span class=&#34;c&#34;&gt;# Add any other local networks here.&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# Now we have two options. Please uncomment out one of them&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;# 1) Redirect packets on port 80 to the webcache&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;#    This may not work unless the webcache is generous with its input&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;#iptables -t nat -A HTTPFORCE -p tcp --dport 80 -j DNAT --to $WEBCACHE&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# 2) Redirect packets on port 80 to localhost port 1234&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;#    On port 1234 you need to run a local web proxy, which forwards &lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;#    requests to the real webcache&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;#iptables -t nat -A HTTPFORCE -p tcp --dport 80 -j REDIRECT --to-port 1234&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# Capture all outgoing TCP syns&lt;/span&gt;
iptables -t nat -A OUTPUT -p tcp --syn -j HTTPFORCE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All outgoing TCP packets on port 80 which are not destined for a local network are captured and changed in one of two ways. The first option just manipulates the IP/TCP header, and the second redirects the traffic to a proxy running on localhost. The reason for the two options was that my university&amp;#8217;s webcache seemed unable to deal with HTTP requests without the full URL in the GET line, even though the request contains a valid Host header. I think this is a misconfiguration of their squid proxy, so instead you can redirect to a local proxy which forwards the request on to the webcache.&lt;/p&gt;

&lt;p&gt;This all seems a hassle but sometimes it is needed when a application does not respect the http_proxy environment. On a good note all libcurl applications should respect it by default.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>