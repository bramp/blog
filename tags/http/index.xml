<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Http on bramp.net</title>
    <link>http://bramp.net/blog/tags/http/</link>
    <description>Recent content in Http on bramp.net</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Tue, 27 Nov 2012 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://bramp.net/blog/tags/http/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Invalid IP range checking defeated by DNS</title>
      <link>http://bramp.net/blog/2012/11/27/invalid-ip-range-checking-defeated-by-dns/</link>
      <pubDate>Tue, 27 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>http://bramp.net/blog/2012/11/27/invalid-ip-range-checking-defeated-by-dns/</guid>
      <description>&lt;p&gt;I&amp;#8217;ve seen a particular kind of vulnerability in a few different applications but I&amp;#8217;m not sure of an appropriate name for it. So I thought I&amp;#8217;d write about it, and informally call it the &amp;#8220;DNS defeated IP address check&amp;#8221;. Basically, if you have an application that can be used as a proxy, or can be instructed to make web request, you don&amp;#8217;t want it fetching files from internal services.&lt;/p&gt;

&lt;p&gt;For example, there is a simple proxy called &lt;a href=&#34;https://github.com/atmos/camo&#34;&gt;Camo&lt;/a&gt;, which is used to fetch third party images when you need to display them on a SSL secure site. (Read more about Camo on the &lt;a href=&#34;https://github.com/blog/743-sidejack-prevention-phase-3-ssl-proxied-assets&#34;&gt;GitHub blog&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This kind of application can be incorrectly setup such that the application has access to internal servers and resources that wouldn&amp;#8217;t normally be exposed to the Internet. This make the proxy application a good way a hacker could gain information about a private network. However Camo tries to address this issue by forbidding URLs that contain private IP addresses. It does a check like so:&lt;/p&gt;

&lt;pre&gt;RESTRICTED_IPS = /^((10\.)|(127\.)|(169\.254)|(192\.168)|(172\.((1[6-9])|(2[0-9])|(3[0-1]))))/

if (url.host.match(RESTRICTED_IPS))
  return four_oh_four(resp, &#34;Hitting excluded hostnames&#34;)
&lt;/pre&gt;

&lt;p&gt;This code (written for &lt;a href=&#34;http://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; in &lt;a href=&#34;http://coffeescript.org/&#34;&gt;CoffeeScript&lt;/a&gt;) is taking a &lt;a href=&#34;http://nodejs.org/api/url.html&#34;&gt;url object&lt;/a&gt; and checking the hostname doesn&amp;#8217;t match a restricted address. This works great against URLs such as &lt;a href=&#34;http://127.0.0.1/&#34;&gt;http://127.0.0.1/&lt;/a&gt;, or &lt;a href=&#34;http://10.0.0.1/&#34;&gt;http://10.0.0.1/&lt;/a&gt;, however this check can easily be defeated. If you create a domain name, such as localhost.bramp.net, which resolves to 127.0.0.1, and ask the proxy to fetch &lt;a href=&#34;http://localhost.bramp.net/&#34;&gt;http://localhost.bramp.net/&lt;/a&gt;, then it won&amp;#8217;t be caught by that check. Now the proxy will continue to try and fetch a resource from 127.0.0.1.&lt;/p&gt;

&lt;p&gt;The solution to this problem is to do that IP address check &lt;strong&gt;after&lt;/strong&gt; the DNS name has been resolved. This can also be problematic if you use a standard library for making web requests, as they will do the DNS lookup for you, and don&amp;#8217;t give you the fine grain control you need. For example, I&amp;#8217;ve seen this be a problem for a Java application using the &lt;a href=&#34;http://hc.apache.org/httpclient-3.x/&#34;&gt;Apache HTTP Client&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One might naively assume they could do a DNS check, and then hand the processing to a HTTP library to make the actual request. The issue here is that the DNS record the HTTP library uses might not be the same as the one you checked against with the DNS check. For example, many domains have multiple A records, and some DNS servers can be configured to round robin DNS records. If you can&amp;#8217;t be sure the HTTP library will do another DNS requests, then you&amp;#8217;d be vulnerable.&lt;/p&gt;

&lt;p&gt;Luckily, in Camo&amp;#8217;s case the fix was relatively easy (see my &lt;a href=&#34;https://github.com/atmos/camo/pull/19&#34;&gt;pull request&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;# We do DNS lookup ourselves
Dns.lookup url.host, (err, address, family) -&gt;
  if address.match(RESTRICTED_IPS)
    return four_oh_four(resp, &#34;Hitting excluded hostnames&#34;)

  # We connect to the IP address, not hostname
  src = Http.createClient url.port || 80, address

  # We add a host header, so the request will work
  headers = 
    &#34;Host&#39; : url.host

  # Boom, we make the request
  srcReq = src.request &#39;GET&#39;, query_path, headers
&lt;/pre&gt;

&lt;p&gt;The above code was simplified a little from the real code, but basically we do the DNS lookup, check the returned address is good, and then make a HTTP request to that IP address with a &lt;code&gt;Host:&lt;/code&gt; header to ensure the request will work.&lt;/p&gt;

&lt;p&gt;Really though, the correct solution to this is to configure a suitably paranoid firewall to stop requests from the proxy machine to anything internal. However, as with all security, the more [layers of protection][8] you have the better, and you should never depend on just one.&lt;/p&gt;

&lt;p&gt;[8]: &lt;a href=&#34;http://en.wikipedia.org/wiki/Swiss_cheese_model&#34;&gt;http://en.wikipedia.org/wiki/Swiss_cheese_model&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Follow HTTP Stream (with decompression)</title>
      <link>http://bramp.net/blog/2010/01/10/follow-http-stream-with-decompression/</link>
      <pubDate>Sun, 10 Jan 2010 00:00:00 +0000</pubDate>
      
      <guid>http://bramp.net/blog/2010/01/10/follow-http-stream-with-decompression/</guid>
      <description>&lt;p&gt;I was using &lt;a href=&#34;http://www.wireshark.org/&#34;&gt;Wireshark&lt;/a&gt; to capture an exchange of HTTP packets, however, some of the HTTP responses were using &amp;#8220;content-encoding: gzip&amp;#8221;, which meant I couldn&amp;#8217;t view them decompressed in the &amp;#8220;Follow TCP Stream&amp;#8221;. Wireshark does decompress them in Packet Details view, but it is hard to follow the full stream like this.&lt;/p&gt;

&lt;p&gt;The solution was to write some &lt;a href=&#34;http://www.python.org/&#34;&gt;Python&lt;/a&gt; which made use of the &lt;a href=&#34;http://code.google.com/p/dpkt/&#34;&gt;dpkt library&lt;/a&gt;. My code naively reassembles the TCP flow and then assumes traffic on port 80 is HTTP. Therefore there is much room for improvement, but here is the code anyway.&lt;/p&gt;

&lt;pre class=&#34;prettyprint&#34;&gt;#!/usr/bin/env python
# Turns a pcap file with http gzip compressed data into plain text, making it
# easier to follow.

import dpkt

def tcp_flags(flags):
    ret = &#39;&#39;
    if flags &amp; dpkt.tcp.TH_FIN:
        ret = ret + &#39;F&#39;
    if flags &amp; dpkt.tcp.TH_SYN:
        ret = ret + &#39;S&#39;
    if flags &amp; dpkt.tcp.TH_RST:
        ret = ret + &#39;R&#39;
    if flags &amp; dpkt.tcp.TH_PUSH:
        ret = ret + &#39;P&#39;
    if flags &amp; dpkt.tcp.TH_ACK:
        ret = ret + &#39;A&#39;
    if flags &amp; dpkt.tcp.TH_URG:
        ret = ret + &#39;U&#39;
    if flags &amp; dpkt.tcp.TH_ECE:
        ret = ret + &#39;E&#39;
    if flags &amp; dpkt.tcp.TH_CWR:
        ret = ret + &#39;C&#39;

    return ret

def parse_http_stream(stream):
    while len(stream) &amp;gt; 0:
        if stream[:4] == &#39;HTTP&#39;:
            http = dpkt.http.Response(stream)
            print http.status
        else:
            http = dpkt.http.Request(stream)
            print http.method, http.uri
        stream = stream[len(http):]

def parse_pcap_file(filename):
    # Open the pcap file
    f = open(&#39;market.pcap&#39;, &#39;rb&#39;)
    pcap = dpkt.pcap.Reader(f)
    
    # I need to reassmble the TCP flows before decoding the HTTP
    conn = dict() # Connections with current buffer
    for ts, buf in pcap:
        eth = dpkt.ethernet.Ethernet(buf)
        if eth.type != dpkt.ethernet.ETH_TYPE_IP:
            continue
    
        ip = eth.data
        if ip.p != dpkt.ip.IP_PROTO_TCP:
            continue
    
        tcp = ip.data
    
        tupl = (ip.src, ip.dst, tcp.sport, tcp.dport)
        #print tupl, tcp_flags(tcp.flags)
    
        # Ensure these are in order! TODO change to a defaultdict
        if tupl in conn:
            conn[ tupl ] = conn[ tupl ] + tcp.data
        else:
            conn[ tupl ] = tcp.data
    
        # TODO Check if it is a FIN, if so end the connection
    
        # Try and parse what we have
        try:
            stream = conn[ tupl ]
            if stream[:4] == &#39;HTTP&#39;:
                http = dpkt.http.Response(stream)
                #print http.status
            else:
                http = dpkt.http.Request(stream)
                #print http.method, http.uri
    
            print http
            print

            # If we reached this part an exception hasn&#39;t been thrown
            stream = stream[len(http):]
            if len(stream) == 0:
                del conn[ tupl ]
            else:
                conn[ tupl ] = stream
        except dpkt.UnpackError:
            pass

    f.close()

if __name__ == &#39;__main__&#39;:
    import sys
    if len(sys.argv) &amp;lt;= 1:
        print &#34;%s &amp;lt;pcap filename&amp;gt;&#34; % sys.argv[0]
        sys.exit(2)

    parse_pcap_file(sys.argv[1])
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;, I had to make a couple of changes to the dpkt library, which I have submitted &lt;a href=&#34;http://groups.google.com/group/dpkt/browse_thread/thread/5315199f9749b91a&#34;&gt;back for review&lt;/a&gt;. Those changes can be found in the following patches &lt;a href=&#34;http://bramp.net/blog/patches/dpkt-pcap-snaplen.patch&#34;&gt;1&lt;/a&gt; &lt;a href=&#34;http://bramp.net/blog/patches/dpkt-http-len.patch&#34;&gt;2&lt;/a&gt; [3][7]. I will update this code if/when the patches get accepted.&lt;/p&gt;

&lt;p&gt;[7]: /patches/dpkt-http-gz.patch&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>